"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[6889],{5105:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>s,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module-4-vla/week-11-vla-architecture","title":"Week 11: VLA Architecture","description":"Vision-Language-Action model fundamentals","source":"@site/docs/module-4-vla/week-11-vla-architecture.mdx","sourceDirName":"module-4-vla","slug":"/module-4-vla/week-11-vla-architecture","permalink":"/textbook-hackathon/module-4-vla/week-11-vla-architecture","draft":false,"unlisted":false,"editUrl":"https://github.com/eDeveloper132/textbook-hackathon/tree/main/docs/module-4-vla/week-11-vla-architecture.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Week 11: VLA Architecture","description":"Vision-Language-Action model fundamentals"},"sidebar":"tutorialSidebar","previous":{"title":"Week 10: Synthetic Data","permalink":"/textbook-hackathon/module-3-isaac/week-10-synthetic-data"},"next":{"title":"Week 12: Fine-tuning","permalink":"/textbook-hackathon/module-4-vla/week-12-finetuning"}}');var a=t(4848),r=t(8453);const o={sidebar_position:1,title:"Week 11: VLA Architecture",description:"Vision-Language-Action model fundamentals"},s="Week 11: VLA Model Architecture",d={},l=[{value:"What are VLA Models?",id:"what-are-vla-models",level:2},{value:"Key VLA Models",id:"key-vla-models",level:2},{value:"Architecture Deep Dive",id:"architecture-deep-dive",level:2},{value:"Training Loop",id:"training-loop",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"week-11-vla-model-architecture",children:"Week 11: VLA Model Architecture"})}),"\n",(0,a.jsx)(n.p,{children:"Understanding Vision-Language-Action models for robotics."}),"\n",(0,a.jsx)(n.h2,{id:"what-are-vla-models",children:"What are VLA Models?"}),"\n",(0,a.jsx)(n.mermaid,{value:'graph LR\n    subgraph "Input"\n        V[Vision - Camera]\n        L[Language - Instructions]\n    end\n    \n    subgraph "VLA Model"\n        E1[Vision Encoder]\n        E2[Language Encoder]\n        F[Fusion Layer]\n        P[Policy Head]\n    end\n    \n    subgraph "Output"\n        A[Actions - Joint Commands]\n    end\n    \n    V --\x3e E1\n    L --\x3e E2\n    E1 --\x3e F\n    E2 --\x3e F\n    F --\x3e P\n    P --\x3e A'}),"\n",(0,a.jsx)(n.h2,{id:"key-vla-models",children:"Key VLA Models"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Model"}),(0,a.jsx)(n.th,{children:"Organization"}),(0,a.jsx)(n.th,{children:"Key Innovation"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"RT-1"}),(0,a.jsx)(n.td,{children:"Google"}),(0,a.jsx)(n.td,{children:"Tokenized actions"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"RT-2"}),(0,a.jsx)(n.td,{children:"Google"}),(0,a.jsx)(n.td,{children:"VLM backbone"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"PaLM-E"}),(0,a.jsx)(n.td,{children:"Google"}),(0,a.jsx)(n.td,{children:"Embodied reasoning"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"OpenVLA"}),(0,a.jsx)(n.td,{children:"Stanford"}),(0,a.jsx)(n.td,{children:"Open weights"})]})]})]}),"\n",(0,a.jsx)(n.h2,{id:"architecture-deep-dive",children:"Architecture Deep Dive"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nfrom transformers import CLIPModel, CLIPProcessor\n\nclass SimpleVLA(nn.Module):\n    def __init__(self, action_dim=7):\n        super().__init__()\n        \n        # Vision-Language backbone\n        self.clip = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")\n        \n        # Freeze CLIP\n        for param in self.clip.parameters():\n            param.requires_grad = False\n        \n        # Action head\n        self.action_head = nn.Sequential(\n            nn.Linear(512 + 512, 256),\n            nn.ReLU(),\n            nn.Linear(256, action_dim)\n        )\n    \n    def forward(self, image, text):\n        # Get embeddings\n        vision_features = self.clip.get_image_features(image)\n        text_features = self.clip.get_text_features(text)\n        \n        # Concatenate\n        combined = torch.cat([vision_features, text_features], dim=-1)\n        \n        # Predict actions\n        actions = self.action_head(combined)\n        return actions\n'})}),"\n",(0,a.jsx)(n.h2,{id:"training-loop",children:"Training Loop"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"def train_step(model, batch, optimizer):\n    images, instructions, target_actions = batch\n    \n    # Forward pass\n    predicted_actions = model(images, instructions)\n    \n    # MSE loss for continuous actions\n    loss = F.mse_loss(predicted_actions, target_actions)\n    \n    # Backward pass\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    return loss.item()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"VLA models"})," combine vision, language, and action"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Foundation models"})," provide strong priors"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Action tokenization"})," enables sequence modeling"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Multi-task training"})," improves generalization"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>s});var i=t(6540);const a={},r=i.createContext(a);function o(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);